# ğŸ§  Observational Memory for OpenClaw

**Give your AI agent humanlike long-term memory â€” no RAG, no embeddings, no databases.**

Two background agents (Observer + Reflector) continuously compress your conversation history into dense, prioritized memory files. Your agent reads these on startup and instantly has full context about you, your projects, your preferences, and what happened while it was "asleep."

Achieves **5â€“40Ã— compression** over raw conversation history while preserving what matters: facts, preferences, decisions, emotional tone, and project context. Scored **SOTA on [LongMemEval](https://arxiv.org/abs/2410.10813)**.

> Inspired by [Mastra's Observational Memory](https://mastra.ai/docs/memory/observational-memory) â€” adapted and extended for the [OpenClaw](https://openclaw.ai) ecosystem.

---

## How It Works

```
  Conversation                     Observations                   Reflections
  (raw messages)                   (compressed notes)             (long-term memory)
                                                                  
  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  Observer       â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  Reflector   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
  â”‚ Hey, can you â”‚  (every 15m)   â”‚ ğŸ”´ 14:30 Userâ”‚  (daily)     â”‚ ## Identity  â”‚
  â”‚ help me set  â”‚ â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–º  â”‚ setting up   â”‚ â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–º  â”‚ Name: Alex   â”‚
  â”‚ up Postgres  â”‚                â”‚ PostgreSQL   â”‚              â”‚ Role: Backendâ”‚
  â”‚ for the new  â”‚                â”‚ for project  â”‚              â”‚ dev          â”‚
  â”‚ project? I   â”‚                â”‚ "Atlas." Pre-â”‚              â”‚              â”‚
  â”‚ tried SQLite â”‚                â”‚ fers Postgresâ”‚              â”‚ ## Projects  â”‚
  â”‚ but it can't â”‚                â”‚ over SQLite  â”‚              â”‚ Atlas: Migra-â”‚
  â”‚ handle the   â”‚                â”‚ for concurr- â”‚              â”‚ ted SQLite â†’ â”‚
  â”‚ concurrency  â”‚                â”‚ ency reasons.â”‚              â”‚ PostgreSQL   â”‚
  â”‚ we need...   â”‚                â”‚              â”‚              â”‚              â”‚
  â”‚ [200+ more   â”‚                â”‚ ğŸŸ¡ 14:45 De- â”‚              â”‚ ## Prefs     â”‚
  â”‚  messages]   â”‚                â”‚ bugging conn â”‚              â”‚ ğŸ”´ Prefers   â”‚
  â”‚              â”‚                â”‚ pool config  â”‚              â”‚ Postgres for â”‚
  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜              â”‚ prod workloadsâ”‚
                                                                â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
  ~50K tokens                      ~2K tokens                    ~500 tokens
```

**Three tiers of memory, each more compressed than the last:**

| Tier | Updated | Retention | Size | Contents |
|------|---------|-----------|------|----------|
| **Raw Messages** | Real-time | Session only | ~50K tokens/day | Full conversation |
| **Observations** | Every 15 min | 7 days | ~2K tokens/day | Timestamped, prioritized notes |
| **Reflections** | Daily | Indefinite | 200â€“600 lines total | Stable identity, projects, preferences |

---

## Example Output

### Observations (`memory/observations.md`)

```markdown
# Observations

## 2026-02-10

### Current Context
- **Active task:** Migrating Atlas project from SQLite to PostgreSQL
- **Mood/tone:** Focused, slightly frustrated with connection pooling
- **Key entities:** Atlas, PostgreSQL, PgBouncer, Render.com
- **Suggested next:** Help verify connection pool settings work under load

### Observations
- ğŸ”´ 14:30 User is migrating the Atlas project from SQLite to PostgreSQL
  - ğŸ”´ 14:30 Reason: SQLite can't handle the concurrent writes they need
  - ğŸŸ¡ 14:35 Using Render.com managed PostgreSQL instance
- ğŸ”´ 14:42 User prefers PostgreSQL over SQLite for production workloads
- ğŸŸ¡ 14:45 Debugging connection pool exhaustion â€” PgBouncer max_client_conn was set too low
  - ğŸŸ¡ 14:52 Resolved: increased to 200 connections, switched to transaction mode
- ğŸŸ¢ 15:00 Quick weather check â€” no follow-up needed
- ğŸ”´ 15:10 User wants to add full-text search to Atlas
  - ğŸŸ¡ 15:10 Considering pg_trgm vs tsvector â€” leaning toward tsvector
```

### Reflections (`memory/reflections.md`)

```markdown
# Reflections â€” Long-Term Memory

*Last updated: 2026-02-10 04:00 UTC*

## Core Identity
- **Name:** Alex Chen
- **Role:** Backend engineer at a Series B startup
- **Communication style:** Direct, technical, appreciates concise answers
- **Working hours:** ~09:00â€“18:00 PST, occasional evening sessions
- **Preferences:** PostgreSQL, Python, FastAPI, prefers CLI over GUI
- **Pet peeves:** Verbose explanations when a code snippet would do

## Active Projects

### Atlas
- **Status:** Active
- **Started:** ~Jan 2026
- **Stack:** Python, FastAPI, PostgreSQL (migrated from SQLite ~Feb 2026)
- **Key decisions:** PostgreSQL for concurrency; PgBouncer in transaction mode; tsvector for search
- **Current state:** Database migration complete, adding full-text search

## Preferences & Opinions
- ğŸ”´ PostgreSQL over SQLite for anything production
- ğŸ”´ Prefers code examples over explanations
- ğŸ”´ Uses Render.com for managed infrastructure
- ğŸŸ¡ Interested in PgBouncer vs pgpool â€” chose PgBouncer for simplicity

## Relationship & Communication
- Likes brief, direct answers â€” expand only when asked
- Appreciates when the agent catches potential issues proactively
- Uses humor when frustrated (dry/sarcastic)
```

---

## Quick Start

### Prerequisites

- [OpenClaw](https://openclaw.ai) installed and running
- `openclaw` CLI available in your PATH

### Install

```bash
git clone https://github.com/intertwine/observational-memory.git
cd observational-memory
bash scripts/install.sh
```

This will:
1. Create `memory/observations.md` and `memory/reflections.md` in your workspace
2. Set up two cron jobs: Observer (every 15 min) and Reflector (daily at 04:00 UTC)

### Configure

```bash
# Custom model
bash scripts/install.sh --model anthropic/claude-sonnet-4-20250514

# Custom schedule
bash scripts/install.sh --observer-interval "*/30 * * * *"  # every 30 min
bash scripts/install.sh --reflector-schedule "0 6 * * *"     # 06:00 UTC daily

# Uninstall
bash scripts/uninstall.sh
bash scripts/uninstall.sh --purge  # also removes memory files
```

### Wire Up Your Agent

Add these lines to your `AGENTS.md` (or equivalent startup instructions):

```markdown
## Every Session
...
5. Read `memory/observations.md` â€” recent compressed observations (auto-maintained by Observer)
6. Read `memory/reflections.md` â€” long-term condensed memory (auto-maintained by Reflector)
```

That's it. Your agent now has persistent, compressed memory.

---

## Enhanced Search with QMD (Optional)

Observational Memory solves the **writing** problem â€” compressing raw conversation into dense, searchable memory files. But what about the **reading** problem? When your agent needs to recall something from weeks ago, how does it find the right observation?

[QMD](https://github.com/nicholasgriffintn/qmd) is a local-first hybrid search engine that combines **BM25** (keyword matching), **vector embeddings** (semantic similarity), and **reranking** (relevance scoring) into a single search pipeline. It's the perfect complement to Observational Memory:

```
  Conversation        Observer          observations.md        QMD indexes
  (raw messages)      (every 15m)       (compressed notes)     (BM25 + vectors)
                                                               
  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”     â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”      â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
  â”‚ 200+ messagesâ”‚â”€â”€â–ºâ”‚ Compress  â”‚â”€â”€â”€â”€â–ºâ”‚ ~2K tokens   â”‚â”€â”€â”€â”€â”€â–ºâ”‚ BM25 index   â”‚
  â”‚ per day      â”‚   â”‚ & score   â”‚     â”‚ per day      â”‚      â”‚ Vector embed â”‚
  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜     â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜      â”‚ Reranker     â”‚
                                                              â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”˜
                                                                     â”‚
                                                              memory_search
                                                              "What was that
                                                               Postgres issue?"
                                                                     â”‚
                                                              â”Œâ”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”
                                                              â”‚ Top 6 resultsâ”‚
                                                              â”‚ with citationsâ”‚
                                                              â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### Why They Work Well Together

Compressed observations are **better search targets** than raw conversation. Instead of searching through thousands of noisy messages, QMD searches through dense, pre-scored notes with clear timestamps and priority levels. This means:

- **Higher precision** â€” observations strip filler, keeping only facts and decisions
- **Better embeddings** â€” dense text produces more meaningful vectors than "hey can you help me with..."
- **Faster indexing** â€” 2K tokens/day vs 50K tokens/day

### OM Works Great Without QMD

QMD is entirely optional. Without it, your agent still reads `observations.md` and `reflections.md` on startup and has full context. QMD adds the ability to **search** across weeks or months of observations when the agent needs to recall something specific.

If QMD is unavailable or fails, OpenClaw automatically falls back to its built-in vector search â€” so there's no hard dependency.

### Resource Requirements

QMD runs locally and uses GGUF models for embedding and reranking:

- **Full setup:** ~2 GB RAM for local GGUF models (embedding + reranking)
- **Lighter setup:** BM25 keyword search works with zero extra RAM; vector embeddings fall back to OpenAI API if GGUF models can't load
- **Disk:** ~1 GB for model files on first run

On smaller VMs (< 4 GB RAM), QMD gracefully degrades â€” BM25 still works, and you get hybrid search with API-based embeddings instead of local ones.

### Enable QMD

```bash
bash scripts/enable-qmd.sh
```

See the [enable script](scripts/enable-qmd.sh) for details, or run `bash scripts/enable-qmd.sh --help`.

To disable and revert to the default memory backend:

```bash
bash scripts/enable-qmd.sh --disable
```

---

## Architecture

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  every 15 min   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚   Main Agent    â”‚ â—„â”€â”€ reads â”€â”€â”€â”€â”€ â”‚  Observer Agent   â”‚
â”‚   Session       â”‚                 â”‚  (cron, isolated) â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”˜                 â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
         â”‚                                   â”‚ writes
         â”‚ reads on startup          â”Œâ”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
         â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–ºâ”‚ memory/           â”‚
                                     â”‚  observations.md  â”‚
                                     â”‚  reflections.md   â”‚
                                     â””â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                                             â”‚ reads + trims
                                     â”Œâ”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                                     â”‚ Reflector Agent   â”‚
                                     â”‚ (daily cron)      â”‚
                                     â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### Observer Agent
- Runs as an OpenClaw cron job (default: every 15 minutes)
- Reads recent session history from the main agent session
- Compresses unprocessed messages into timestamped, prioritized notes
- Appends to `memory/observations.md`
- Maintains a "Current Context" block with active tasks, mood, and suggested next actions

### Reflector Agent
- Runs daily (default: 04:00 UTC)
- Reads all observations and existing reflections
- Merges, promotes, demotes, and archives entries based on frequency and recency
- Overwrites `memory/reflections.md` with a clean, condensed long-term memory document
- Trims observations older than 7 days

### Priority System

| Level | Meaning | Examples | Retention |
|-------|---------|----------|-----------|
| ğŸ”´ | Important / persistent | User facts, decisions, project architecture | Months+ |
| ğŸŸ¡ | Contextual | Current tasks, in-progress work, open questions | Daysâ€“weeks |
| ğŸŸ¢ | Minor / transient | Greetings, routine checks, small talk | Hours |

---

## Customization

### Tuning Compression

Edit the prompts in `reference/` to adjust:
- **What gets captured** â€” modify the priority definitions in `observer-prompt.md`
- **How aggressively observations are merged** â€” adjust the merge/promote/archive rules in `reflector-prompt.md`
- **Target size** â€” the reflector aims for 200â€“600 lines; change this in the prompt

### Adjusting Frequency

```bash
openclaw cron list                    # see current jobs
openclaw cron edit observer-memory    # modify observer schedule
openclaw cron edit reflector-memory   # modify reflector schedule
```

### Manual Triggers

```bash
openclaw cron trigger observer-memory    # run observer now
openclaw cron trigger reflector-memory   # run reflector now
```

### Model Selection

The install script defaults to `anthropic/claude-sonnet-4-20250514`. Both agents work well with any capable model â€” Sonnet-class or better is recommended for the observer, while the reflector benefits from stronger reasoning (Opus-class) for complex merging decisions.

---

## File Structure

```
observational-memory/
â”œâ”€â”€ README.md              # This file
â”œâ”€â”€ LICENSE                # MIT
â”œâ”€â”€ SKILL.md               # OpenClaw skill integration guide
â”œâ”€â”€ reference/
â”‚   â”œâ”€â”€ observer-prompt.md # System prompt for the Observer agent
â”‚   â””â”€â”€ reflector-prompt.md# System prompt for the Reflector agent
â””â”€â”€ scripts/
    â”œâ”€â”€ install.sh         # Automated setup
    â”œâ”€â”€ uninstall.sh       # Clean removal
    â””â”€â”€ enable-qmd.sh     # Optional: enable QMD hybrid search
```

---

## FAQ

**Q: Does this replace RAG / vector search?**
A: For personal assistant use cases, yes. Observational memory works better for remembering *about a person* â€” their preferences, projects, communication style. RAG is better for searching large document collections. They're complementary.

**Q: How much does it cost to run?**
A: The observer processes only new messages each run (~100â€“500 input tokens typical). The reflector reads more but runs only once daily. Expect ~$0.05â€“0.20/day with Sonnet-class models.

**Q: What if the observer misses something?**
A: The observer errs on the side of keeping observations ("when in doubt, keep it"). The reflector handles cleanup. You can also manually edit `memory/observations.md` at any time.

**Q: Can I use this outside OpenClaw?**
A: The prompts are generic and work with any agent framework that supports cron-like scheduling and file-based memory. The install script is OpenClaw-specific, but the pattern is portable.

---

## Credits

- **Inspired by** [Mastra's Observational Memory](https://mastra.ai/docs/memory/observational-memory) â€” the original OM pattern that achieved SOTA on LongMemEval
- **Built for** the [OpenClaw](https://openclaw.ai) community
- **License:** MIT â€” fork it, customize it, ship it

---

*Made with ğŸ§  by [Intertwine](https://github.com/intertwine)*
